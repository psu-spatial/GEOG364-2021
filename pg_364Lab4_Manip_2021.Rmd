---
title: "Lab 4: Manipulating Spatial Data"
subtitle: <h4 style="font-style:normal">GEOG-364 - Spatial Analysis</h4>
output: 
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
    theme: flatly
---

<style>
p.comment {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 0px;
border-radius: 5px;
font-style: normal;
}

h1.title {
  font-weight: bold;
  font-family: Arial;  
}

h2.title {
  font-family: Arial;  
}

</style>


<style type="text/css">
#TOC {
  font-size: 11px;
  font-family: Arial;
}
</style>

\
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message = FALSE)
```


```{r,include=FALSE,echo=FALSE}
# invisible data read
library(tidyverse)
library(sp)
library(sf)
library(readxl)
library(skimr)
library(tmap)
library(USAboundaries)
library(viridis)
library(rnaturalearth)
library(kableExtra)


frost   <- readxl::read_excel("pg_364Data_1frostday.xlsx")
newyork <- readxl::read_excel("pg_364Data_1frostday.xlsx")
firefly <- readxl::read_excel("pg_364Data_3Firefly.xlsx")
ozone   <- read.csv("pg_364Data_4Ozone.csv")
```

## Welcome to Lab 4!

<br>

The aim of this lab is start getting more comfortable making high quality maps and manipulating data.  We will also learn about some new markdown techniques for "inline code" and adding equations By the end of the lab you will be able to.

 - 1. Use inline code commands
 - 2. Insert an equation into your text and data
 - 3. Make beautiful spatial plots
 - 4. Read in shapefiles and manipulate your data spatially
 - 5. Re-analyse your new york data

Assignment 4 is due by midnight the night before your next lab on Canvas. Your job is to submit the requirements on this page.

See [**your canvas assignment here**](https://psu.instructure.com/courses/2120046/assignments/13274836).  E.g go to canvas for assignment guidelines.

<p class="comment">**Need help?**  Add a screenshot/question to the discussion board here:    [**LAB 4 DISCUSSION BOARD**](https://psu.instructure.com/courses/2120046/discussion_topics/14125712)</p>

<br>
<br>

<div style="margin-bottom:25px;">
</div>  
## A: Set up the lab


Set up R-Studio in the same way as before, creating a project file and a blank markdown document.<br>This will be the last time I am going to spell it out.

1. **Step 1**<br>Open R-Studio.<br>Create a new R-Project in your GEOG-364 folder called `GEOG364_Lab4_PROJECT`.<br>*Reminder: [Tutorial 2C: Projects](https://psu-spatial.github.io/Geog364-2021/pg_Tut2_startup.html)*.   
  

<br>  

2. **Step 2**<br>Go on Canvas to the Lab 4 page and download any datasets as needed (`ozone.csv`) <br>Put it/them in your `GEOG364_Lab4_PROJECT` folder.  See the lab 3 instructions for how it should look.

<br>

3. **Step 3**<br>Make a copy of the lab template: 
    a. *If you're on your own computer:*<br>
       + Make a **COPY** of the template Rmd file.
       <br>
       + Rename the **COPY** `GEOG364_Lab4_userID_CODE.Rmd` e.g. for me GEOG364_Lab3_hlg5155_CODE.Rmd
       <br>
       + Move GEOG364_Lab4_userID_CODE.Rmd into your GEOG364_Lab4_PROJECT folder.
       <br>
    
    <br>
    
    b. *If you're using R-studio cloud.*<br>
       + MAKE SURE YOU WENT TO YOUR WORKSPACE AND MADE A LAB 4 PROJECT!
       + Click on your Lab 4 project folder
       <br>
       + In the files tab, click the check box by your template file, click More/Export & download it.
       <br>
       + Click on Workspaces (top left) and enter your Lab 4 project folder.
       <br>
       + In the files tab, click upload and upload the template
       <br>
       + Now click the check box by the template file and rename to `GEOG364_Lab4_userID_CODE.Rmd`
       <br> e.g. for me GEOG364_Lab4_hlg5155_CODE.Rmd<br>

<br>

In both cases, your "Lab 4" R-Studio should look like this but for lab 4.  If not ASK FOR HELP

```{r, Lab4Fig31, echo=FALSE, fig.align='center',out.width="100%", fig.cap="*This is for lab 3 but you get the idea.*"}
knitr::include_graphics('pg_364Lab3_Spatial_2021_fig2.png')
```

<br>

<p class="comment">**IMPORTANT! Did you open R via your lab 4 project file?** If R-studio *isn't* "looking in" your lab 4 folder in Files, or doesn't have your lab 4 project name on the logo/at the top of the screen, close R studio, go to your lab 4 folder and double click the .Rproj to open your lab 4 project</p>

<br>

4. **Step 4**<br>Click on your lab 4 .Rmd file in the files tab to open the script<br>(You should not have to search if you are in your Lab 4 project):<br>
     + Change the title to Lab 4.    
     + Change the theme if you like     
     + Edit the library code chunk so it looks like this:
       ```{r,eval=FALSE}
       library(knitr) 
       library(tidyverse)
       library(sf)
       library(sp)
       library(tmap)
       library(skimr)
       library(rnaturalearth)
       library(readxl)
       ```
     + Add in message=FALSE and warning=FALSE inside the top code chunk (see screenshot below) to remove the packages printing all the text
     + If you accidentally messed up something remember you can re-copy your YAML code or options from a previous lab.<br>*The easiest way to do this is to navigate on your computer to that lab folder then double click the project file for the old lab to open an entirely new version of R-Studio for that lab. Then nothing gets confused*<br>*equally on the cloud, make a new tab in your browser, go to your workspace (top left), and open the other project there*<br>     
     + Press knit.

<br>   

You screen should now look like this but for Lab 4 and your options.  If so,congrats<br>If not, ASK FOR HELP.  

```{r, Lab4FigL22, echo=FALSE, fig.align='center',fig.cap="*Note, your message=FALSE and warning = FALSE can go in the general options like my screenshot, or in the code chunk options for the library chunk*"}
knitr::include_graphics('pg_364Lab3_Spatial_2021_fig3.png')
```

<br>

<div style="margin-bottom:25px;">
</div>  
## B: Markdown Edits

We are now going to insert "inline code" along with mathematical equations:

5. **Step 5: Visual editor**<br>Read the section: [Tutorial 4G: Visual Editor](https://psu-spatial.github.io/Geog364-2021/pg_Tut4_markdown.html#Tutorial_4G:_Formatting_text) to turn on the markdown visual editor (near the top of 4G)

<br>

6. **Step 6: Inline code**<br>Follow the new inline-code tutorial in [Tutorial 4F: Inline Code](https://psu-spatial.github.io/Geog364-2021/pg_Tut4_markdown.html#Tutorial_4F:_Formatting_code_chunks) to understand about inline code

<br> 

7. **Step 7: Equations**<br>Follow [Tutorial 4I: Equations](https://psu-spatial.github.io/Geog364-2021/pg_Tut4_markdown.html#Tutorial_4I:_Adding_equations) to understand about adding equations.

<br> 

8. **Step 8:**<br> Make a new section called Markdown.<br>*Feel free to use subsections, numbers, font formats throughout to make it look professional as possible<br> IF YOU SKIPPED STEPS 6,7,8, GO DO THEM.* 

<br>


9. **Step 9:**<br>Now, attempt the following tasks, which are building on those tutorials but also on what you learnt in Labs 1,2 & 3:<br>[TUTORIAL 4: MARKDOWN](https://psu-spatial.github.io/Geog364-2021/pg_Tut4_markdown.html#Tutorial_4G:_Formatting_text) HAS THE ANSWERS.

   a. **Visual editor**<br>Write a short paragraph to explain the difference between pattern and process ([**See Lecture 5A & Sullivan chapter pdf**](https://psu.instructure.com/courses/2120046/pages/lecture-5a-maup-pattern-vs-process)).<br>Use the visual editor to play with the fonts & formats.<br>*[you are being graded on the paragraph content not the format, so go wild]*<br><br>
   b. **Inline code**
      + COPY your frost data from Lab 2 to your Lab 4 folder.    
      + In a code chunk, read in the data to a variable called frost.  
      + In the same code chunk, calculate 
         + *the mean of the Elevation column*
         + *the number of rows*  
         + *the number of columns( hint `nrow() and ncol()`)*
         + Save the answers to sensibly named variables e.g. `frost.elev`,`frost.nrow`....<br>
      + Edit the code chunk options so that the code and output are invisible when you press knit (you will still see them in the editing window).<br><br>
      + In the text make a new paragraph, that says<br>_*"In lab 2, we read in a dataset called frostdata.xlsx.  The data has XXXX rows and YYYY columns,  The mean of the Elevation column is ZZZZ UNITS."*_.<br><br>Replace XXXX, YYYY and ZZZZ with inline code that creates the answers automatically and UNITS with the appropriate units.<br><br>   
   c. **Rounding**<br> Your mean elevation is to too many decimal places in your sentence.<br>Use the  `round()` command to round your output to a sensible number of significant figures.<br>To do this:<br> 
      + See this tutorials below on round (or google)  https://www.datasciencemadesimple.com/round-function-in-r/
      + Make a new copy of your sentence about frost, 
      + Now edit the inline code chunk to round the mean of the elevation to an appropriate number of decimal places for your units.<br>
      + Your final sentence should look something link this but about your frost data and with a good number of decimal places for your units.<br><br>*"In the inbuilt starwars dataset, there are `r nrow(starwars)` rows and  `r ncol(starwars)` columns.  The average height of a character is `r round(mean(starwars$height,na.rm=TRUE),1)` cm."*
   d. **Equations**<br> In a new paragraph, describe the Z score and include the equation properly formatted.<br>
   
   
<br>

<div style="margin-bottom:25px;">
</div>  
## C: Better spatial plots

<br> 

This is the final "lego" lab e.g. where it is more about building blocks than statistics.  So let's improve our maps. There are some wonderful tutorials online, much better than I could write.  So we shall work through one of them

10. **Step 10:**<br>Tutorial script.<br>We need a place for you to work through the tutorial.  Follow step 3 to make a new copy of your lab template and put it in the Lab 4 folder.**This time, call your copy  `GEOG364_MapTutorial.Rmd`**

<p class="comment">**You do NOT have to submit your tutorial file**</p>

11. **Step 11:**<br>Work through the tutorial.<br>Go here: https://mgimond.github.io/Spatial/mapping-data-in-r.html <br> Work through the mapping tutorial at that page, copying the code into code chunks in your tutorial lab script, checking it works and trying to understand why each line of code is there.

12. **Step 12:**<br> Once you are done, close your tutorial script so that you don't accidentally edit it for your Lab 4. 

<br>

<div style="margin-bottom:25px;">
</div>  
## D: Ozone & MAUP

### Lab Background

<span style="color:#134f5c">As cities increase in size, it is  important to understand atmospheric air pollution. Two major pollutants include Ozone (O3) and particulates (PM 2.5). In this lab, we will map them and explore their impact on populations in California.</span>

<span style="color:#134f5c">Ozone is a naturally occurring molecule made up of three Oxygen atoms. 90% of the plant’s ozone exists in the “ozone layer”, which exists approximately 20-25km above sea level, in the stratosphere. Stratospheric ozone is crucial for human life, as it absorbs damaging solar ultra-violet radiation (see the “ozone hole” for the importance of natural ozone).</span>

<span style="color:#134f5c">However, 10% of the planet’s ozone is found in the lowest levels of the atmosphere, in the air which we breathe. It is created when pollutants such as NOx and VOCs (emitted by cars and industry) react with sunlight.</span>

<span style="color:#134f5c">Strong evidence exists that the risk of premature death increases with increased amounts of low-level-ozone.Too much ozone can cause immediate health problems such as asthma. Over the longer term, ozone exposure can exacerbate pre-existing lung and heart health problems.  It is therefore important to map the spread of ozone to support public health policy.</span> 

<span style="color:#134f5c">To explore this issue, ozone measurements were taken across the State of California. Population density has also been recorded at each site using this source: https://sedac.ciesin.columbia.edu/data/set/gpw-v4-population-density-rev11.</span>

 - <span style="color:#134f5c">*The units of the ozone measurements are in “1000 parts per billion”.</span>
 - <span style="color:#134f5c">*The units of population density are people per square km.</span>


<br>
<br>

### Basics

13. **Step 13:**<br>Read the `ozone.csv` data into R and assign to a variable called `ozone`.<br>*hint! Like HW4 you need a the `read.csv()` command instead of the read_excel one. Tutorial here if you're stuck (https://www.datacamp.com/community/tutorials/r-data-import-tutorial)*

<br>

14. **Step 14: Summary Statistics...**<br>This is week 4, you know the drill! Explore the data and tell me professionally in the text:<br>

   a. Tell me more about Ozone pollution and health.  What's the big deal (feel free to use REFERENCED pictures/screenshots)<br><br>
   b. Tell me what you know about the data/study.  Imagine you are a journalist or a policy maker - be critical!  If some aspect of the "meta data" (the data source, author, population etc.) is missing, make sure to note it.<br><br>
   b. Tell me about the data you just read into R e.g.
      + What is the unit of observation for this data
      + Is the data marked?  If so, what are the marks? (e.g. what columns are there)
      + How many observations are there? (e.g. how many rows)
      + What are the summary statistics - do they look reasonable?
      + What does the histograms of the ozone column look like (remember to get the exact column name you can use the command `names(ozonedata)`)?<br>
      + Use a QQ-Norm to plot to assess if the data in the ozone column is normally distributed. (hint, see the QQQplot here http://www.sthda.com/english/wiki/normality-test-in-r if you want a prettier one)
      + BONUS!  Conduct and interpret in full sentences, a Wilks-Shapiro test to assess if the data is normal (hint: tutorial: http://www.sthda.com/english/wiki/normality-test-in-r)

<br>

15. **Step 15: Research hypothesis...**<br>From what you (now) know about ozone, how do you expect the pattern of ozone to be distributed over California?  What influences it and what are confounding factors?


```{r, include=FALSE}
ozone.sf <- st_as_sf(ozone,coords=c("LONGITUDE","LATITUDE"),crs=4326)
```


### Spatial mapping


16. **Step 16: Convert to spatial...**<br>Make a new code chunk<br>Use *Reminder: [Tutorial 11Bb convert-to-sf: Projects](https://psu-spatial.github.io/Geog364-2021/pg_Tut11_spatial101.html#b_Converting_a_dataframe_in_R_to_spatial_sf)* to do the following:
   a. Look at the ozone data and note the colunm names containing the x and y coordinates (CASE SENSITIVE)<br>
   b. Convert `ozone` to sf format and assign the result to a variable called `ozone.sf`<br>*hint, the current map projection is Lat/Long 4326*<br><br>
   c. Check it works. use the command `plot(st_geometry(ozone.sf))` to make sure it converted OK.  Hint, to understand what I was asking last week with st_geometry, also try `plot(ozone.sf)`  and `plot(ozone$LONGITUDE,ozone$LATITUDE)`
   
<br>  

16. **Step 17: First map...**<br>Use the previous labs and tutorials to make an interactive map of ozone levels. Use the layers button to explore the data.  Does the distribution agree with what you expected? (hint, rural Joshua Tree has some of the highest Ozone levels..)
   
   
<br>   

```{r,include=FALSE}
   ca_congress.sf <- us_boundaries(states="CA", type = "congressional")
   ca_counties.sf <- us_boundaries(states="CA", type = "county")
```


18. **Step 18: Download county borders**<br>Make a new code chunk<br>Use this code to download county borders for California and plot to check (ignore the warnings). Luckily for us they are in the same map projection. you should see something like this:

   ```{r}  
   ca_counties.sf <- us_boundaries(states="CA", type = "county")
   
   plot(st_geometry(ca_counties.sf))
   plot(st_geometry(ozone.sf),add=TRUE,col="red",pch=16,cex=.5)
   ```

19. **Step 19: Download congressional borders**<br>Make a new code chunk<br>Use this code to download congressional borders for California and plot to check (ignore the warnings). Luckily for us they are in the same map projection. you should see something like this:

   ```{r}  
   ca_congress.sf <- us_boundaries(states="CA", type = "congressional")
   
   plot(st_geometry(ca_congress.sf))
   plot(st_geometry(ozone.sf),add=TRUE,col="red",pch=16,cex=.5)
   ```

Here are my prettier tmaps of population density over ozone tracts.

```{r}
# Plot the population density data, the col is the column name
# You can run the command tmaptools::palette_explorer() in the console and choose a more appropriate palette (you will need to install the shinyjs package)
tmap_mode("view")
tm_shape(ca_congress.sf) + tm_borders() +
  tm_shape(ozone.sf) +
  tm_dots(col="POPULATION_DENSITY", palette = "YlOrRd", 
          title="Population density (people/km^2)", size=0.04) +
  tm_legend(legend.outside=TRUE)+
  tm_basemap(leaflet::providers$OpenStreetMap.DE)
```

```{r}
# Plot the population density data, the col is the column name
# No basemaps are possible in static mode, which is frustrating
tmap_mode("plot")
tm_shape(ca_congress.sf) + tm_borders() +
  tm_shape(ozone.sf) +
  tm_dots(col="POPULATION_DENSITY", palette = "YlOrRd", 
          title="Population density (people/km^2)", size=0.04) +
  tm_legend(legend.outside=TRUE)
```


20. **Step 20: Make prettier maps**<br> Given my code here and your tutorials, make some better maps of the ozone levels at your California sites, overlayed with the COUNTY borders.<br>(hint the column name is OZONE_1000PPB..)


### Spatial wrangling

Finally, let's assess how to summarise our data across polygons using the st_join command

Putting this into a tutorial broke the website, so let's add it here.


22. **Step 22:Get it running on your computer**<br> Copy the code below into a code chunk in your lab script.  Run each line and see if you can understand what is going on. Check it knits and that you can see the same maps in your lab output.

```{r}
# First, look at the ozone column names to understand that table
head(ozone.sf)
```



```{r}
#----------------------------------------------------------
# Now look at the ca_congress.sf column names, 
# You can see there are a load of random columns
# From googling, I worked out that the congressional 
#   district column is cd115fp
#-----------------------------------------------------------
head(ca_congress.sf)
```
```{r, include=FALSE}
#-----------------------------------------------------------
# We are first going to stick these together to make a table where I know the 
# zone every point is in. 
#-----------------------------------------------------------
joineddata  <- st_join(ozone.sf, ca_counties.sf)

#-----------------------------------------------------------
# Now I'm going to do a load of statistics on the joined data
#-----------------------------------------------------------

# How many observations in each congressional district
number.obs    <-  joineddata %>%  
                   group_by(geoid) %>% 
                   summarize(number.obs = length(geoid)) %>% 
                   select(number.obs)

# Mean population density in each congressional district
mean_POPULATION_DENSITY <-   joineddata %>%  
                             group_by(geoid) %>% 
                             summarize(mean_POPULATION_DENSITY = mean(POPULATION_DENSITY)) %>% 
                             select(mean_POPULATION_DENSITY)
                   
# Max population density in each congressional district
max_POPULATION_DENSITY <-    joineddata %>%  
                             group_by(geoid) %>% 
                             summarize(max_POPULATION_DENSITY = max(POPULATION_DENSITY)) %>% 
                             select(max_POPULATION_DENSITY)

# Max population density in each congressional district
mean_OZONE_1000PPB    <-   joineddata %>%  
                           group_by(geoid) %>% 
                           summarize(mean_OZONE_1000PPB = mean(OZONE_1000PPB)) %>% 
                           select(mean_OZONE_1000PPB)

#-----------------------------------------------------------
# And finally, merge them with the congressional polygon variable
# I'm first making a copy so that I don't mess anything up
#-----------------------------------------------------------
ca_county.data.sf <- ca_counties.sf

ca_county.data.sf <- st_join(ca_county.data.sf, number.obs)
ca_county.data.sf <- st_join(ca_county.data.sf, mean_POPULATION_DENSITY)
ca_county.data.sf <- st_join(ca_county.data.sf, max_POPULATION_DENSITY)
ca_county.data.sf <- st_join(ca_county.data.sf, mean_OZONE_1000PPB)

#plot3 <- qtm(ca_county.data.sf,fill ="mean_OZONE_1000PPB")
#plot4 <- qtm(ca_congress.data.sf,fill ="mean_OZONE_1000PPB")
#tmap_arrange(plot3,plot4)


```

```{r}
#-----------------------------------------------------------
# We are first going to stick these together to make a table where I know the 
# zone every point is in. 
#-----------------------------------------------------------
joineddata  <- st_join(ozone.sf, ca_congress.sf)

#-----------------------------------------------------------
# Now I'm going to do a load of statistics on the joined data
#-----------------------------------------------------------

# How many observations in each congressional district
number.obs    <-  joineddata %>%  
                   group_by(cd115fp) %>% 
                   summarize(number.obs = length(cd115fp)) %>% 
                   select(number.obs)

# Mean population density in each congressional district
mean_POPULATION_DENSITY <-   joineddata %>%  
                             group_by(cd115fp) %>% 
                             summarize(mean_POPULATION_DENSITY = mean(POPULATION_DENSITY)) %>% 
                             select(mean_POPULATION_DENSITY)
                   
# Max population density in each congressional district
max_POPULATION_DENSITY <-    joineddata %>%  
                             group_by(cd115fp) %>% 
                             summarize(max_POPULATION_DENSITY = max(POPULATION_DENSITY)) %>% 
                             select(max_POPULATION_DENSITY)

# Max population density in each congressional district
mean_OZONE_1000PPB    <-   joineddata %>%  
                           group_by(cd115fp) %>% 
                           summarize(mean_OZONE_1000PPB = mean(OZONE_1000PPB)) %>% 
                           select(mean_OZONE_1000PPB)

#-----------------------------------------------------------
# And finally, merge them with the congressional polygon variable
# I'm first making a copy so that I don't mess anything up
#-----------------------------------------------------------
ca_congress.data.sf <- ca_congress.sf

ca_congress.data.sf <- st_join(ca_congress.data.sf, number.obs)
ca_congress.data.sf <- st_join(ca_congress.data.sf, mean_POPULATION_DENSITY)
ca_congress.data.sf <- st_join(ca_congress.data.sf, max_POPULATION_DENSITY)
ca_congress.data.sf <- st_join(ca_congress.data.sf, mean_OZONE_1000PPB)

#-----------------------------------------------------------
# And test plot, note I'm now plotting ca_congress.data.sf!
#-----------------------------------------------------------
plot1 <- qtm(ca_congress.data.sf,fill ="mean_POPULATION_DENSITY")
plot2 <- qtm(ca_congress.data.sf,fill ="max_POPULATION_DENSITY")
tmap_arrange(plot1,plot2)

plot3 <- qtm(ca_congress.data.sf,fill ="number.obs")
plot4 <- qtm(ca_congress.data.sf,fill ="mean_OZONE_1000PPB")
tmap_arrange(plot3,plot4)

```

```{r}
#-----------------------------------------------------------
# And finally take a look at the column names of the new sf variable - you can see
# our calculations have been added in
#-----------------------------------------------------------
head(ca_congress.data.sf)
```

<br>
<br>

23. **Step 23:Do the same for the county polygons**<br>Copy/paste the code you just got running into a new code chunk.<br>In THAT ONE, edit the code so that it runs for county borders rather than for the congressional districts and save the result as `ca_counties.data.sf`.<br>
    + *How to do it:* 
    +  *The county data is stored as `ca_counties.sf`, so replace all your `ca_congress.data.sf` with `ca_counties.sf`* 
    + *The unique column name is `geoid`, so replace all your `cd115fp` with `geoid`* 
    + *The output sf variable is `ca_county.data.sf`. so replace all your `ca_congress.data.sf` with `ca_county.data.sf`*<br>
    
<p class="comment">**Hint for fast copy/paste**  Double click on the variable name you want to copy, it should highlight it. Press command-c (or ctrl-c on a windows).  Double click on the variable you want to replace and press command-v | ctrl-v </p>

<br>

24.  **Step 24: BONUS.**<br>Use what you have just learned to find and quickly plot the maximum ozone level in each county in California (aggregated from your point data).


25. **Step 25: Better plots**<br> You should now have sf variables called `ca_counties.data.sf` and `ca_congress.data.sf`, within which there are columns called` mean_OZONE_1000PPB`.<br>You also just did a T-map tutorial on good looking polygon maps.<br>Make a map of the county aggregated mean Ozone levels, and another for the congressional district aggregated mean Ozone levels.  Show me all the cool things you can do.<br> See if you can use tmap_arrange to plot them side by side.

<br>

26. **Step 26: Explain what you did**<br> I want to make better tmap tutorials, so below the maps explain what tmap options you chose and why.

<br>

27. **Step 27: MAUP**: During this lab, you have now created three maps of ozone levels over California: [1] At a point level, [2] Aggregated over counties and [3] aggregated over congressional districts.<br> Use those three plots to <br>
  - Explain the Modifiable Areal Unit problem, especially the scale and zone effects
  - Explain how do the different aggregations could change your perceptions of the processes causing low level ozone and the people most at risk?.
  
<br>

28. **Step 28: Raster to vector**: A big problem with this data is that Ozone is a raster (e.g. anywhere on Earth you can measure ozone levels). However, the data we had *sampled* out of the raster layer at specific point locations.<br>Explain how the distribution of the measurement point locations impacted the Modifiable Areal Unit Problem effect you described above.

<br<


## E. Above and beyond

Remember that an A is 93%, so you can ignore this section and still easily get an A. But here is your time to shine. Also, if you are struggling in another part of the lab, you can use this to gain back points.

**To get the final 4 marks in the lab, you need to show me something new, e.g. you need to go above and beyond the lab questions in some way.** 

 - You get 2/4 for doing something new in any way (including the Wilks Shapiro or step 24)
 - You get 4/4 for something really impressive or 3+ small things.
 
Please tell us in your R script what you did!



## F. Submitting your Lab

Remember to save your work throughout and to spell check your writing (left of the knit button). Now, press the knit button again. If you have not made any mistakes in the code then R should create a html file in your lab 4 folder which includes your answers. If you look at your lab 4 folder, you should see this there - complete with a very recent time-stamp.

In that folder, double click on the html file.  This will open it in your browser. CHECK THAT THIS IS WHAT YOU WANT TO SUBMIT

Now go to Canvas and submit BOTH your html and your .Rmd file in Lab 4.

<br>

## Lab 4 submission check-list

**For all answers: Full marks = everything down at a high standard, in full sentences as appropriate with no parts of your answer missing. Imagine it as an example I use in class**


**HTML FILE SUBMISSION - 5 marks**

**RMD CODE SUBMISSION - 5 marks**

**MARKDOWN/CODE STYLE - 16 MARKS**

Your code and document is neat and easy to read. LOOK AT YOUR HTML FILE IN YOUR WEB-BROWSER BEFORE YOU SUBMIT.You have written in full sentences, it is clear what your answers are referring to. YOU HAVE USED THE SPELL CHECK. SPELLING ERRORS LOSE YOU MARKS.
    
**MARKDOWN LEVEL-UP: 15 MARKS** 

You have successfully added equations, inline code and followed question 9.<br>  

**OZONE:Summary Statistics 15 MARKS** 

You fully the dataset and conducted appropriate summary statistics.<br>

**OZONE:Research hypothesis 5 MARKS** 

You came up with a reasonable hypothesis for what you imagine the map/pattern of ozone looks like over CA. (Q.15) <br>

**OZONE:Mapping points 10 MARKS** 

You managed to read in all the spatial data and made professional looking point plots, then reassessed your hypothesis <br>

**OZONE:Mapping points 5 MARKS** 

You managed to read in all the spatial data and made professional looking point plots, then reassessed your hypothesis <br>

**OZONE:Aggregating data 10 MARKS** 

You managed to create county aggregated data and made professional looking plots (Q23-Q26) <br>

**OZONE:MAUP  10 MARKS** 

You thoughtfully answered questions 27 and 28 <br>


**Above and beyond: 4 MARKS**

You get 2/4 for doing something new in any way and 4/4 for something really impressive or multiple small things.


[100 marks total]


Overall, here is what your lab should correspond to:

```{r, echo=FALSE}
rubric <- readxl::read_excel("pg_364Lab_rubrictable.xlsx")
knitr::kable(rubric) %>%   
  kable_classic_2() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"))


```








***

Website created and maintained by [Helen Greatrex](https://www.geog.psu.edu/directory/helen-greatrex). Website template by [Noli Brazil](https://nbrazil.faculty.ucdavis.edu/)